{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install textdescriptives"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcoPbb24Xc6M",
        "outputId": "01dc9dca-d847-49c9-bc19-05c32bd4284b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textdescriptives in /usr/local/lib/python3.11/dist-packages (2.8.4)\n",
            "Requirement already satisfied: spacy>=3.6.0 in /usr/local/lib/python3.11/dist-packages (from spacy[lookups]>=3.6.0->textdescriptives) (3.8.5)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from textdescriptives) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from textdescriptives) (2.2.2)\n",
            "Requirement already satisfied: pyphen>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from textdescriptives) (0.17.2)\n",
            "Requirement already satisfied: ftfy>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from textdescriptives) (6.3.1)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from textdescriptives) (2.11.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy>=6.0.3->textdescriptives) (0.2.13)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->textdescriptives) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->textdescriptives) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->textdescriptives) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->textdescriptives) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->textdescriptives) (2.33.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->textdescriptives) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->textdescriptives) (0.4.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.6.0->spacy[lookups]>=3.6.0->textdescriptives) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.6.0->spacy[lookups]>=3.6.0->textdescriptives) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.6.0->spacy[lookups]>=3.6.0->textdescriptives) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.6.0->spacy[lookups]>=3.6.0->textdescriptives) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.6.0->spacy[lookups]>=3.6.0->textdescriptives) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.6.0->spacy[lookups]>=3.6.0->textdescriptives) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.6.0->spacy[lookups]>=3.6.0->textdescriptives) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.6.0->spacy[lookups]>=3.6.0->textdescriptives) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.6.0->spacy[lookups]>=3.6.0->textdescriptives) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.6.0->spacy[lookups]>=3.6.0->textdescriptives) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.6.0->spacy[lookups]>=3.6.0->textdescriptives) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.6.0->spacy[lookups]>=3.6.0->textdescriptives) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.6.0->spacy[lookups]>=3.6.0->textdescriptives) (2.32.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.6.0->spacy[lookups]>=3.6.0->textdescriptives) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy>=3.6.0->spacy[lookups]>=3.6.0->textdescriptives) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.6.0->spacy[lookups]>=3.6.0->textdescriptives) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.6.0->spacy[lookups]>=3.6.0->textdescriptives) (3.5.0)\n",
            "Requirement already satisfied: spacy_lookups_data<1.1.0,>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from spacy[lookups]>=3.6.0->textdescriptives) (1.0.5)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy>=3.6.0->spacy[lookups]>=3.6.0->textdescriptives) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->textdescriptives) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.6.0->spacy[lookups]>=3.6.0->textdescriptives) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.6.0->spacy[lookups]>=3.6.0->textdescriptives) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.6.0->spacy[lookups]>=3.6.0->textdescriptives) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.6.0->spacy[lookups]>=3.6.0->textdescriptives) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy>=3.6.0->spacy[lookups]>=3.6.0->textdescriptives) (1.2.1)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy>=3.6.0->spacy[lookups]>=3.6.0->textdescriptives) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3.6.0->spacy[lookups]>=3.6.0->textdescriptives) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3.6.0->spacy[lookups]>=3.6.0->textdescriptives) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3.6.0->spacy[lookups]>=3.6.0->textdescriptives) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.6.0->spacy[lookups]>=3.6.0->textdescriptives) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.6.0->spacy[lookups]>=3.6.0->textdescriptives) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy>=3.6.0->spacy[lookups]>=3.6.0->textdescriptives) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=3.6.0->spacy[lookups]>=3.6.0->textdescriptives) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.6.0->spacy[lookups]>=3.6.0->textdescriptives) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.6.0->spacy[lookups]>=3.6.0->textdescriptives) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy>=3.6.0->spacy[lookups]>=3.6.0->textdescriptives) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.6.0->spacy[lookups]>=3.6.0->textdescriptives) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler # Added StandardScaler\n",
        "from sklearn.decomposition import PCA # Added PCA\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import ast # Used for safely evaluating string representations of dicts\n",
        "import numpy as np # Import numpy to handle potential NaNs after parsing\n",
        "import matplotlib.pyplot as plt # Added for plotting\n",
        "import seaborn as sns # Added for heatmap\n",
        "\n",
        "# --- Configuration ---\n",
        "# Specify the path to the final processed CSV file (output of evaluate_prompts.py)\n",
        "PROCESSED_FILE_PATH = 'textdescriptives_processed_prompt_examples_dataset.csv'\n",
        "TARGET_COLUMN = 'prompt_qual'\n",
        "TEST_SIZE = 0.2 # Proportion of data to use for testing\n",
        "RANDOM_STATE = 42 # For reproducibility\n",
        "PCA_N_COMPONENTS = None # Set to an int (e.g., 10) or float (e.g., 0.95 for variance) or None to keep all\n",
        "\n",
        "# --- Load Data ---\n",
        "print(f\"Loading data from: {PROCESSED_FILE_PATH}\")\n",
        "try:\n",
        "    # Explicitly tell pandas to keep 'nan' as a string for now, not interpret it as NaN yet\n",
        "    df = pd.read_csv(PROCESSED_FILE_PATH, keep_default_na=False, na_values=[''])\n",
        "    print(\"Data loaded successfully.\")\n",
        "    print(\"Initial DataFrame info:\")\n",
        "    df.info()\n",
        "    print(\"\\nFirst 5 rows:\")\n",
        "    print(df.head())\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {PROCESSED_FILE_PATH}\")\n",
        "    print(\"Please ensure 'extract_data.py' and 'evaluate_prompts.py' have been run successfully.\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the data: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- Feature Engineering & Selection ---\n",
        "print(\"\\nStarting feature engineering...\")\n",
        "\n",
        "# Define potential feature columns generated by textdescriptives\n",
        "potential_feature_cols = [\n",
        "    \"readability\", \"token_length\", \"sentence_length\",\n",
        "    \"coherence\", \"information_theory\", \"entropy\",\n",
        "    \"perplexity\", \"per_word_perplexity\"\n",
        "]\n",
        "\n",
        "# Identify columns that actually exist in the DataFrame\n",
        "existing_feature_cols = [col for col in potential_feature_cols if col in df.columns]\n",
        "print(f\"Found potential feature columns: {existing_feature_cols}\")\n",
        "\n",
        "# Flatten dictionary-like columns\n",
        "features_list = []\n",
        "original_df_index = df.index # Keep track of original index for joining later\n",
        "\n",
        "for col in existing_feature_cols:\n",
        "    # Ensure we're working with a clean copy and avoid SettingWithCopyWarning\n",
        "    col_data = df[col].copy()\n",
        "\n",
        "    # Check if the column contains string representations of dictionaries\n",
        "    # Use dropna().iloc[0] to safely get the first non-NA value for type checking\n",
        "    first_valid_value = col_data.dropna().iloc[0] if not col_data.dropna().empty else None\n",
        "\n",
        "    if isinstance(first_valid_value, str) and first_valid_value.strip().startswith('{'):\n",
        "        print(f\"Processing string-dict column: {col}\")\n",
        "        try:\n",
        "            # --- FIX: Replace 'nan' with 'None' before parsing ---\n",
        "            processed_col_series = col_data.fillna('').astype(str).str.replace(r'\\bnan\\b', 'None', regex=True)\n",
        "\n",
        "            # Define a helper function to safely apply literal_eval\n",
        "            def safe_literal_eval(item):\n",
        "                try:\n",
        "                    if not item or item == 'None': return None\n",
        "                    return ast.literal_eval(item)\n",
        "                except (ValueError, SyntaxError):\n",
        "                    # print(f\"Warning: Could not parse item in column '{col}': {item}. Returning None.\") # Can be verbose\n",
        "                    return None\n",
        "                except Exception as e:\n",
        "                    # print(f\"Warning: Unexpected error parsing item in column '{col}': {item}. Error: {e}. Returning None.\") # Can be verbose\n",
        "                    return None\n",
        "\n",
        "            # Apply the safe evaluation function\n",
        "            expanded_col = processed_col_series.apply(safe_literal_eval)\n",
        "\n",
        "            # Normalize the dictionary into separate columns\n",
        "            normalized_df = pd.json_normalize(expanded_col[expanded_col.notna()])\n",
        "            normalized_df = normalized_df.reindex(original_df_index)\n",
        "            normalized_df.columns = [f\"{col}_{sub_col}\" for sub_col in normalized_df.columns]\n",
        "            features_list.append(normalized_df)\n",
        "            print(f\"Successfully flattened string-dict column: {col}\")\n",
        "\n",
        "        except Exception as e:\n",
        "             print(f\"Warning: An unexpected error occurred processing column '{col}': {e}. Skipping.\")\n",
        "\n",
        "    # Check if the column contains actual dictionary objects\n",
        "    elif isinstance(first_valid_value, dict):\n",
        "        print(f\"Processing dict column: {col}\")\n",
        "        try:\n",
        "            normalized_df = pd.json_normalize(col_data)\n",
        "            normalized_df = normalized_df.reindex(original_df_index)\n",
        "            normalized_df.columns = [f\"{col}_{sub_col}\" for sub_col in normalized_df.columns]\n",
        "            features_list.append(normalized_df)\n",
        "            print(f\"Successfully flattened dict column: {col}\")\n",
        "        except Exception as e:\n",
        "             print(f\"Warning: An unexpected error occurred processing dict column '{col}': {e}. Skipping.\")\n",
        "\n",
        "    # Assume it's a simple numeric column\n",
        "    elif pd.api.types.is_numeric_dtype(col_data):\n",
        "        print(f\"Using numeric column directly: {col}\")\n",
        "        features_list.append(col_data.to_frame().reindex(original_df_index))\n",
        "    else:\n",
        "        print(f\"Warning: Column '{col}' type ({type(first_valid_value)}) is not numeric or a recognized dictionary format. Skipping.\")\n",
        "\n",
        "\n",
        "# Combine all processed features\n",
        "if not features_list:\n",
        "    print(\"Error: No valid features found after processing. Exiting.\")\n",
        "    exit()\n",
        "\n",
        "X = pd.concat(features_list, axis=1)\n",
        "\n",
        "# --- Handle Missing Values (Impute with mean) ---\n",
        "if X.isnull().sum().sum() > 0:\n",
        "    print(\"\\nWarning: Missing values found in features. Imputing with mean.\")\n",
        "    # print(X.isnull().sum()) # Can be verbose\n",
        "    numeric_cols = X.select_dtypes(include=np.number).columns\n",
        "    for col in numeric_cols:\n",
        "        if X[col].isnull().any():\n",
        "            mean_val = X[col].mean()\n",
        "            if pd.isna(mean_val):\n",
        "                mean_val = 0\n",
        "                # print(f\"Warning: Mean for column {col} is NaN. Imputing with 0.\") # Can be verbose\n",
        "            X[col].fillna(mean_val, inplace=True)\n",
        "            # print(f\"Imputed numeric column: {col} with mean {mean_val:.4f}\") # Can be verbose\n",
        "    non_numeric_cols = X.select_dtypes(exclude=np.number).columns\n",
        "    if not X[non_numeric_cols].isnull().sum().sum() == 0:\n",
        "         print(\"Warning: Non-numeric NaNs detected after processing. Consider specific handling.\")\n",
        "else:\n",
        "    print(\"\\nNo missing values found in features.\")\n",
        "\n",
        "\n",
        "print(\"\\nFinal features prepared:\")\n",
        "X.info()\n",
        "print(X.head())\n",
        "\n",
        "# --- Correlation Analysis ---\n",
        "print(\"\\n--- Correlation Matrix Analysis ---\")\n",
        "# Ensure only numeric columns are used for correlation\n",
        "X_numeric = X.select_dtypes(include=np.number)\n",
        "if X_numeric.shape[1] < X.shape[1]:\n",
        "    print(f\"Warning: Excluded {X.shape[1] - X_numeric.shape[1]} non-numeric columns from correlation analysis.\")\n",
        "\n",
        "if X_numeric.shape[1] > 1: # Need at least 2 numeric columns for correlation\n",
        "    correlation_matrix = X_numeric.corr()\n",
        "    print(\"Correlation Matrix calculated.\")\n",
        "\n",
        "    # Display the matrix (optional, can be large)\n",
        "    # print(correlation_matrix)\n",
        "\n",
        "    # Visualize the correlation matrix using a heatmap\n",
        "    plt.figure(figsize=(12, 10)) # Adjust size as needed\n",
        "    sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', fmt=\".2f\") # annot=True can be slow/cluttered for many features\n",
        "    plt.title('Feature Correlation Matrix')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    # plt.show() # Display the plot interactively\n",
        "    # Instead of plt.show(), save the figure if running non-interactively\n",
        "    try:\n",
        "        corr_matrix_path = \"feature_correlation_matrix.png\"\n",
        "        plt.savefig(corr_matrix_path)\n",
        "        print(f\"Correlation matrix heatmap saved to {corr_matrix_path}\")\n",
        "        plt.close() # Close the plot figure\n",
        "    except Exception as e:\n",
        "        print(f\"Could not save correlation matrix heatmap: {e}\")\n",
        "        plt.close() # Ensure figure is closed even on error\n",
        "else:\n",
        "    print(\"Skipping correlation matrix: Not enough numeric features.\")\n",
        "\n",
        "\n",
        "# --- Principal Component Analysis (PCA) ---\n",
        "print(\"\\n--- Principal Component Analysis (PCA) ---\")\n",
        "# PCA requires features to be scaled\n",
        "if X_numeric.shape[1] > 0: # Need at least 1 numeric column for PCA\n",
        "    print(\"Scaling numeric features for PCA...\")\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X_numeric)\n",
        "    print(\"Features scaled.\")\n",
        "\n",
        "    print(f\"Performing PCA (n_components={PCA_N_COMPONENTS})...\")\n",
        "    pca = PCA(n_components=PCA_N_COMPONENTS, random_state=RANDOM_STATE)\n",
        "    pca.fit(X_scaled)\n",
        "\n",
        "    # Explained variance\n",
        "    explained_variance_ratio = pca.explained_variance_ratio_\n",
        "    cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
        "    n_components_fitted = pca.n_components_\n",
        "\n",
        "    print(f\"\\nPCA finished. Number of components fitted: {n_components_fitted}\")\n",
        "    print(\"Explained Variance Ratio per Component:\")\n",
        "    for i, ratio in enumerate(explained_variance_ratio):\n",
        "        print(f\"  PC {i+1}: {ratio:.4f}\")\n",
        "\n",
        "    print(\"\\nCumulative Explained Variance Ratio:\")\n",
        "    for i, ratio in enumerate(cumulative_variance_ratio):\n",
        "        print(f\"  Up to PC {i+1}: {ratio:.4f}\")\n",
        "\n",
        "    # Optional: Plot cumulative explained variance\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(range(1, n_components_fitted + 1), cumulative_variance_ratio, marker='o', linestyle='--')\n",
        "    plt.title('Cumulative Explained Variance by PCA Components')\n",
        "    plt.xlabel('Number of Components')\n",
        "    plt.ylabel('Cumulative Explained Variance Ratio')\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    # plt.show() # Display the plot interactively\n",
        "    # Save the figure\n",
        "    try:\n",
        "        pca_variance_path = \"pca_explained_variance.png\"\n",
        "        plt.savefig(pca_variance_path)\n",
        "        print(f\"PCA explained variance plot saved to {pca_variance_path}\")\n",
        "        plt.close()\n",
        "    except Exception as e:\n",
        "        print(f\"Could not save PCA variance plot: {e}\")\n",
        "        plt.close()\n",
        "\n",
        "    # Optional: You can transform the data if needed for other purposes\n",
        "    # X_pca = pca.transform(X_scaled)\n",
        "    # print(f\"\\nData transformed to PCA components shape: {X_pca.shape}\")\n",
        "\n",
        "else:\n",
        "    print(\"Skipping PCA: No numeric features available.\")\n",
        "\n",
        "\n",
        "# --- Target Variable Preparation ---\n",
        "print(f\"\\n--- Preparing Target Variable ---\")\n",
        "if TARGET_COLUMN not in df.columns:\n",
        "    print(f\"Error: Target column '{TARGET_COLUMN}' not found in the DataFrame.\")\n",
        "    exit()\n",
        "\n",
        "y = df[TARGET_COLUMN]\n",
        "\n",
        "# Encode target labels ('bad' -> 0, 'good' -> 1)\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "print(f\"Target variable encoded: {list(le.classes_)} -> {list(range(len(le.classes_)))}\")\n",
        "# print(f\"Value counts:\\n{pd.Series(y_encoded).value_counts()}\") # Can be verbose\n",
        "\n",
        "\n",
        "# --- Data Splitting ---\n",
        "print(f\"\\n--- Data Splitting ---\")\n",
        "# Ensure X and y_encoded have the same index before splitting\n",
        "if not X.index.equals(pd.RangeIndex(start=0, stop=len(y_encoded), step=1)):\n",
        "     print(\"Warning: Resetting index on X and y_encoded before splitting to ensure alignment.\")\n",
        "     X = X.reset_index(drop=True)\n",
        "     if len(X) != len(y_encoded):\n",
        "         print(f\"Error: Length mismatch after processing. X length: {len(X)}, y_encoded length: {len(y_encoded)}. Exiting.\")\n",
        "         exit()\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_encoded, # Use original X for XGBoost\n",
        "    test_size=TEST_SIZE,\n",
        "    random_state=RANDOM_STATE,\n",
        "    stratify=y_encoded\n",
        ")\n",
        "print(f\"Training set shape: X={X_train.shape}, y={y_train.shape}\")\n",
        "print(f\"Testing set shape: X={X_test.shape}, y={y_test.shape}\")\n",
        "\n",
        "# --- Model Training ---\n",
        "print(\"\\n--- Model Training ---\")\n",
        "# Initialize XGBoost classifier\n",
        "xgb_clf = xgb.XGBClassifier(\n",
        "    objective='binary:logistic',\n",
        "    eval_metric='logloss',\n",
        "    use_label_encoder=False,\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=3,\n",
        "    random_state=RANDOM_STATE,\n",
        "    validate_parameters=True\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "xgb_clf.fit(X_train, y_train)\n",
        "print(\"Model training complete.\")\n",
        "\n",
        "# --- Model Evaluation ---\n",
        "print(\"\\n--- Model Evaluation ---\")\n",
        "y_pred = xgb_clf.predict(X_test)\n",
        "y_pred_proba = xgb_clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(pd.DataFrame(cm, index=le.classes_, columns=[f\"Predicted {c}\" for c in le.classes_]))\n",
        "\n",
        "# --- Feature Importance (Optional) ---\n",
        "try:\n",
        "    print(\"\\n--- Feature Importances (XGBoost) ---\")\n",
        "    importances = pd.DataFrame({\n",
        "        'Feature': X.columns, # Use columns from original X\n",
        "        'Importance': xgb_clf.feature_importances_\n",
        "    }).sort_values(by='Importance', ascending=False)\n",
        "    with pd.option_context('display.max_rows', None):\n",
        "        print(importances)\n",
        "except Exception as e:\n",
        "    print(f\"Could not display feature importances: {e}\")\n",
        "\n",
        "joblib.dump(X.columns, 'feature_columns.joblib')\n",
        "joblib.dump(xgb_clf, 'xgb_model.joblib')\n",
        "joblib.dump(le, 'label_encoder.joblib')\n",
        "print(\"\\n--- Pipeline Finished ---\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data from: textdescriptives_processed_prompt_examples_dataset.csv\n",
            "Data loaded successfully.\n",
            "Initial DataFrame info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2900 entries, 0 to 2899\n",
            "Data columns (total 19 columns):\n",
            " #   Column                Non-Null Count  Dtype  \n",
            "---  ------                --------------  -----  \n",
            " 0   id                    2900 non-null   int64  \n",
            " 1   prompt_example        2900 non-null   object \n",
            " 2   prompt_qual           2900 non-null   object \n",
            " 3   task_description      2900 non-null   object \n",
            " 4   complexity            2900 non-null   object \n",
            " 5   bad_prompt            1450 non-null   object \n",
            " 6   good_prompt           1450 non-null   object \n",
            " 7   expected_answer       2900 non-null   object \n",
            " 8   prompting_techniques  2900 non-null   object \n",
            " 9   prompt_type           2900 non-null   object \n",
            " 10  notes                 0 non-null      float64\n",
            " 11  readability           2900 non-null   object \n",
            " 12  token_length          2900 non-null   object \n",
            " 13  sentence_length       2900 non-null   object \n",
            " 14  coherence             2900 non-null   object \n",
            " 15  information_theory    2900 non-null   object \n",
            " 16  entropy               2900 non-null   float64\n",
            " 17  perplexity            2900 non-null   float64\n",
            " 18  per_word_perplexity   2900 non-null   float64\n",
            "dtypes: float64(4), int64(1), object(14)\n",
            "memory usage: 430.6+ KB\n",
            "\n",
            "First 5 rows:\n",
            "   id                                     prompt_example prompt_qual  \\\n",
            "0   1          Tell me a story about neural prosthetics.         bad   \n",
            "1   2  Imagine you are a science fiction author renow...        good   \n",
            "2   3              Tell me about photography techniques.         bad   \n",
            "3   4  You are an expert photography tutor. I want to...        good   \n",
            "4   5                  Tell me about neural prosthetics.         bad   \n",
            "\n",
            "                                    task_description complexity  \\\n",
            "0  Develop a creative narrative about neural pros...       high   \n",
            "1  Develop a creative narrative about neural pros...       high   \n",
            "2  Decode the logic behind this photography techn...     medium   \n",
            "3  Decode the logic behind this photography techn...     medium   \n",
            "4  Chat about recent developments in neural prost...       high   \n",
            "\n",
            "                                  bad_prompt  \\\n",
            "0                                        NaN   \n",
            "1  Tell me a story about neural prosthetics.   \n",
            "2                                        NaN   \n",
            "3      Tell me about photography techniques.   \n",
            "4                                        NaN   \n",
            "\n",
            "                                         good_prompt  \\\n",
            "0  Imagine you are a science fiction author renow...   \n",
            "1                                                NaN   \n",
            "2  You are an expert photography tutor. I want to...   \n",
            "3                                                NaN   \n",
            "4  You are a leading neuroscientist specializing ...   \n",
            "\n",
            "                                     expected_answer  \\\n",
            "0  The ideal output would be a well-structured an...   \n",
            "1  The ideal output would be a well-structured an...   \n",
            "2  The ideal output would be a Python function th...   \n",
            "3  The ideal output would be a Python function th...   \n",
            "4  The ideal answer would begin with a brief defi...   \n",
            "\n",
            "                     prompting_techniques       prompt_type  notes  \\\n",
            "0  ['ROLE_PROMPTING', 'TREE_OF_THOUGHTS']  CREATIVE_WRITING    NaN   \n",
            "1  ['ROLE_PROMPTING', 'TREE_OF_THOUGHTS']  CREATIVE_WRITING    NaN   \n",
            "2                      ['CODE_PROMPTING']  CODE_EXPLANATION    NaN   \n",
            "3                      ['CODE_PROMPTING']  CODE_EXPLANATION    NaN   \n",
            "4  ['ROLE_PROMPTING', 'CHAIN_OF_THOUGHT']    CONVERSATIONAL    NaN   \n",
            "\n",
            "                                         readability  \\\n",
            "0  {'flesch_reading_ease': 78.87285714285717, 'fl...   \n",
            "1  {'flesch_reading_ease': 52.20517786561268, 'fl...   \n",
            "2  {'flesch_reading_ease': 66.40000000000003, 'fl...   \n",
            "3  {'flesch_reading_ease': 54.17750000000001, 'fl...   \n",
            "4  {'flesch_reading_ease': 66.40000000000003, 'fl...   \n",
            "\n",
            "                                        token_length  \\\n",
            "0  {'token_length_mean': 4.857142857142857, 'toke...   \n",
            "1  {'token_length_mean': 5.440993788819876, 'toke...   \n",
            "2  {'token_length_mean': 6.4, 'token_length_media...   \n",
            "3  {'token_length_mean': 5.325, 'token_length_med...   \n",
            "4  {'token_length_mean': 5.6, 'token_length_media...   \n",
            "\n",
            "                                     sentence_length  \\\n",
            "0  {'sentence_length_mean': 7.0, 'sentence_length...   \n",
            "1  {'sentence_length_mean': 14.636363636363637, '...   \n",
            "2  {'sentence_length_mean': 5.0, 'sentence_length...   \n",
            "3  {'sentence_length_mean': 16.0, 'sentence_lengt...   \n",
            "4  {'sentence_length_mean': 5.0, 'sentence_length...   \n",
            "\n",
            "                                           coherence  \\\n",
            "0  {'first_order_coherence': nan, 'second_order_c...   \n",
            "1  {'first_order_coherence': 0.39588747918605804,...   \n",
            "2  {'first_order_coherence': nan, 'second_order_c...   \n",
            "3  {'first_order_coherence': 0.44024402648210526,...   \n",
            "4  {'first_order_coherence': nan, 'second_order_c...   \n",
            "\n",
            "                                  information_theory   entropy  perplexity  \\\n",
            "0  {'entropy': 0.25532731478804593, 'perplexity':...  0.255327    1.290884   \n",
            "1  {'entropy': 6.588365996090378, 'perplexity': 7...  6.588366  726.592644   \n",
            "2  {'entropy': 0.17624011386133565, 'perplexity':...  0.176240    1.192724   \n",
            "3  {'entropy': 3.205725714395446, 'perplexity': 2...  3.205726   24.673399   \n",
            "4  {'entropy': 0.17604065178909264, 'perplexity':...  0.176041    1.192487   \n",
            "\n",
            "   per_word_perplexity  \n",
            "0             0.161361  \n",
            "1             3.885522  \n",
            "2             0.198787  \n",
            "3             0.232768  \n",
            "4             0.198748  \n",
            "\n",
            "Starting feature engineering...\n",
            "Found potential feature columns: ['readability', 'token_length', 'sentence_length', 'coherence', 'information_theory', 'entropy', 'perplexity', 'per_word_perplexity']\n",
            "Processing string-dict column: readability\n",
            "Successfully flattened string-dict column: readability\n",
            "Processing string-dict column: token_length\n",
            "Successfully flattened string-dict column: token_length\n",
            "Processing string-dict column: sentence_length\n",
            "Successfully flattened string-dict column: sentence_length\n",
            "Processing string-dict column: coherence\n",
            "Successfully flattened string-dict column: coherence\n",
            "Processing string-dict column: information_theory\n",
            "Successfully flattened string-dict column: information_theory\n",
            "Using numeric column directly: entropy\n",
            "Using numeric column directly: perplexity\n",
            "Using numeric column directly: per_word_perplexity\n",
            "\n",
            "Warning: Missing values found in features. Imputing with mean.\n",
            "\n",
            "Final features prepared:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2900 entries, 0 to 2899\n",
            "Data columns (total 22 columns):\n",
            " #   Column                                   Non-Null Count  Dtype  \n",
            "---  ------                                   --------------  -----  \n",
            " 0   readability_flesch_reading_ease          2900 non-null   float64\n",
            " 1   readability_flesch_kincaid_grade         2900 non-null   float64\n",
            " 2   readability_smog                         2900 non-null   float64\n",
            " 3   readability_gunning_fog                  2900 non-null   float64\n",
            " 4   readability_automated_readability_index  2900 non-null   float64\n",
            " 5   readability_coleman_liau_index           2900 non-null   float64\n",
            " 6   readability_lix                          2900 non-null   float64\n",
            " 7   readability_rix                          2900 non-null   float64\n",
            " 8   token_length_token_length_mean           2900 non-null   float64\n",
            " 9   token_length_token_length_median         2900 non-null   float64\n",
            " 10  token_length_token_length_std            2900 non-null   float64\n",
            " 11  sentence_length_sentence_length_mean     2900 non-null   float64\n",
            " 12  sentence_length_sentence_length_median   2900 non-null   float64\n",
            " 13  sentence_length_sentence_length_std      2900 non-null   float64\n",
            " 14  coherence_first_order_coherence          2900 non-null   float64\n",
            " 15  coherence_second_order_coherence         2900 non-null   float64\n",
            " 16  information_theory_entropy               2900 non-null   float64\n",
            " 17  information_theory_perplexity            2900 non-null   float64\n",
            " 18  information_theory_per_word_perplexity   2900 non-null   float64\n",
            " 19  entropy                                  2900 non-null   float64\n",
            " 20  perplexity                               2900 non-null   float64\n",
            " 21  per_word_perplexity                      2900 non-null   float64\n",
            "dtypes: float64(22)\n",
            "memory usage: 498.6 KB\n",
            "   readability_flesch_reading_ease  readability_flesch_kincaid_grade  \\\n",
            "0                        78.872857                          3.997143   \n",
            "1                        52.205178                          9.613834   \n",
            "2                        66.400000                          5.240000   \n",
            "3                        54.177500                          9.677500   \n",
            "4                        66.400000                          5.240000   \n",
            "\n",
            "   readability_smog  readability_gunning_fog  \\\n",
            "0         12.145695                 8.514286   \n",
            "1         12.243490                12.811067   \n",
            "2         12.145695                10.000000   \n",
            "3         11.602472                11.900000   \n",
            "4         12.145695                10.000000   \n",
            "\n",
            "   readability_automated_readability_index  readability_coleman_liau_index  \\\n",
            "0                                 4.947143                        8.531429   \n",
            "1                                11.515263                       14.170683   \n",
            "2                                11.214000                       15.912000   \n",
            "3                                11.650750                       13.661000   \n",
            "4                                 7.446000                       11.208000   \n",
            "\n",
            "   readability_lix  readability_rix  token_length_token_length_mean  \\\n",
            "0        21.285714         1.000000                        4.857143   \n",
            "1        48.176736         4.909091                        5.440994   \n",
            "2        45.000000         2.000000                        6.400000   \n",
            "3        48.500000         5.200000                        5.325000   \n",
            "4        25.000000         1.000000                        5.600000   \n",
            "\n",
            "   token_length_token_length_median  ...  \\\n",
            "0                               5.0  ...   \n",
            "1                               5.0  ...   \n",
            "2                               5.0  ...   \n",
            "3                               5.0  ...   \n",
            "4                               5.0  ...   \n",
            "\n",
            "   sentence_length_sentence_length_median  \\\n",
            "0                                     7.0   \n",
            "1                                    15.0   \n",
            "2                                     5.0   \n",
            "3                                    11.0   \n",
            "4                                     5.0   \n",
            "\n",
            "   sentence_length_sentence_length_std  coherence_first_order_coherence  \\\n",
            "0                             0.000000                         0.431073   \n",
            "1                             4.676299                         0.395887   \n",
            "2                             0.000000                         0.431073   \n",
            "3                            11.296017                         0.440244   \n",
            "4                             0.000000                         0.431073   \n",
            "\n",
            "   coherence_second_order_coherence  information_theory_entropy  \\\n",
            "0                          0.407475                    0.255327   \n",
            "1                          0.441429                    6.588366   \n",
            "2                          0.407475                    0.176240   \n",
            "3                          0.489360                    3.205726   \n",
            "4                          0.407475                    0.176041   \n",
            "\n",
            "   information_theory_perplexity  information_theory_per_word_perplexity  \\\n",
            "0                       1.290884                                0.161361   \n",
            "1                     726.592644                                3.885522   \n",
            "2                       1.192724                                0.198787   \n",
            "3                      24.673399                                0.232768   \n",
            "4                       1.192487                                0.198748   \n",
            "\n",
            "    entropy  perplexity  per_word_perplexity  \n",
            "0  0.255327    1.290884             0.161361  \n",
            "1  6.588366  726.592644             3.885522  \n",
            "2  0.176240    1.192724             0.198787  \n",
            "3  3.205726   24.673399             0.232768  \n",
            "4  0.176041    1.192487             0.198748  \n",
            "\n",
            "[5 rows x 22 columns]\n",
            "\n",
            "--- Correlation Matrix Analysis ---\n",
            "Correlation Matrix calculated.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-014312cc1f13>:133: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X[col].fillna(mean_val, inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correlation matrix heatmap saved to feature_correlation_matrix.png\n",
            "\n",
            "--- Principal Component Analysis (PCA) ---\n",
            "Scaling numeric features for PCA...\n",
            "Features scaled.\n",
            "Performing PCA (n_components=None)...\n",
            "\n",
            "PCA finished. Number of components fitted: 22\n",
            "Explained Variance Ratio per Component:\n",
            "  PC 1: 0.4446\n",
            "  PC 2: 0.2115\n",
            "  PC 3: 0.1046\n",
            "  PC 4: 0.0661\n",
            "  PC 5: 0.0423\n",
            "  PC 6: 0.0353\n",
            "  PC 7: 0.0262\n",
            "  PC 8: 0.0248\n",
            "  PC 9: 0.0141\n",
            "  PC 10: 0.0088\n",
            "  PC 11: 0.0080\n",
            "  PC 12: 0.0053\n",
            "  PC 13: 0.0046\n",
            "  PC 14: 0.0020\n",
            "  PC 15: 0.0010\n",
            "  PC 16: 0.0007\n",
            "  PC 17: 0.0000\n",
            "  PC 18: 0.0000\n",
            "  PC 19: 0.0000\n",
            "  PC 20: 0.0000\n",
            "  PC 21: 0.0000\n",
            "  PC 22: 0.0000\n",
            "\n",
            "Cumulative Explained Variance Ratio:\n",
            "  Up to PC 1: 0.4446\n",
            "  Up to PC 2: 0.6562\n",
            "  Up to PC 3: 0.7607\n",
            "  Up to PC 4: 0.8268\n",
            "  Up to PC 5: 0.8692\n",
            "  Up to PC 6: 0.9045\n",
            "  Up to PC 7: 0.9306\n",
            "  Up to PC 8: 0.9554\n",
            "  Up to PC 9: 0.9695\n",
            "  Up to PC 10: 0.9783\n",
            "  Up to PC 11: 0.9864\n",
            "  Up to PC 12: 0.9917\n",
            "  Up to PC 13: 0.9963\n",
            "  Up to PC 14: 0.9983\n",
            "  Up to PC 15: 0.9993\n",
            "  Up to PC 16: 1.0000\n",
            "  Up to PC 17: 1.0000\n",
            "  Up to PC 18: 1.0000\n",
            "  Up to PC 19: 1.0000\n",
            "  Up to PC 20: 1.0000\n",
            "  Up to PC 21: 1.0000\n",
            "  Up to PC 22: 1.0000\n",
            "PCA explained variance plot saved to pca_explained_variance.png\n",
            "\n",
            "--- Preparing Target Variable ---\n",
            "Target variable encoded: ['bad', 'good'] -> [0, 1]\n",
            "\n",
            "--- Data Splitting ---\n",
            "Training set shape: X=(2320, 22), y=(2320,)\n",
            "Testing set shape: X=(580, 22), y=(580,)\n",
            "\n",
            "--- Model Training ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [20:53:13] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model training complete.\n",
            "\n",
            "--- Model Evaluation ---\n",
            "\n",
            "Accuracy: 0.9862\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         bad       0.99      0.99      0.99       290\n",
            "        good       0.99      0.99      0.99       290\n",
            "\n",
            "    accuracy                           0.99       580\n",
            "   macro avg       0.99      0.99      0.99       580\n",
            "weighted avg       0.99      0.99      0.99       580\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "      Predicted bad  Predicted good\n",
            "bad             286               4\n",
            "good              4             286\n",
            "\n",
            "--- Feature Importances (XGBoost) ---\n",
            "                                    Feature  Importance\n",
            "16               information_theory_entropy    0.569573\n",
            "13      sentence_length_sentence_length_std    0.334469\n",
            "7                           readability_rix    0.011083\n",
            "11     sentence_length_sentence_length_mean    0.010337\n",
            "3                   readability_gunning_fog    0.009633\n",
            "14          coherence_first_order_coherence    0.008181\n",
            "18   information_theory_per_word_perplexity    0.007929\n",
            "8            token_length_token_length_mean    0.006491\n",
            "5            readability_coleman_liau_index    0.006363\n",
            "12   sentence_length_sentence_length_median    0.006223\n",
            "9          token_length_token_length_median    0.006157\n",
            "4   readability_automated_readability_index    0.005831\n",
            "15         coherence_second_order_coherence    0.004560\n",
            "1          readability_flesch_kincaid_grade    0.003960\n",
            "10            token_length_token_length_std    0.003775\n",
            "0           readability_flesch_reading_ease    0.003308\n",
            "6                           readability_lix    0.002125\n",
            "17            information_theory_perplexity    0.000000\n",
            "2                          readability_smog    0.000000\n",
            "19                                  entropy    0.000000\n",
            "20                               perplexity    0.000000\n",
            "21                      per_word_perplexity    0.000000\n",
            "\n",
            "--- Pipeline Finished ---\n"
          ]
        }
      ],
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-Czu3yHLe8y",
        "outputId": "3b61d276-f25e-455a-d3c3-113d63816d45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import textdescriptives as td\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Assume joblib or pickle is used for loading\n",
        "import joblib # Or import pickle\n",
        "\n",
        "# --- 1. Load Prerequisites ---\n",
        "# Load the spacy model with textdescriptives pipe\n",
        "# Make sure you use the same model as in your training script!\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    if \"textdescriptives/all\" not in nlp.pipe_names:\n",
        "        nlp.add_pipe(\"textdescriptives/all\")\n",
        "except OSError:\n",
        "    print(\"Downloading en_core_web_sm...\")\n",
        "    spacy.cli.download(\"en_core_web_sm\")\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    nlp.add_pipe(\"textdescriptives/all\")\n",
        "\n",
        "# !!! Load your trained XGBoost model and LabelEncoder !!!\n",
        "# Replace 'xgb_model.joblib' and 'label_encoder.joblib' with the actual paths\n",
        "# You would need to add saving logic to your main training script first, e.g.:\n",
        "# joblib.dump(xgb_clf, 'xgb_model.joblib')\n",
        "# joblib.dump(le, 'label_encoder.joblib')\n",
        "try:\n",
        "    xgb_model = joblib.load('xgb_model.joblib')\n",
        "    label_encoder = joblib.load('label_encoder.joblib')\n",
        "    # !!! Load the expected feature columns !!!\n",
        "    # You should save these from your training script as well\n",
        "    # e.g., joblib.dump(X.columns, 'feature_columns.joblib')\n",
        "    expected_columns = joblib.load('feature_columns.joblib')\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Model, LabelEncoder or Feature Columns file not found.\")\n",
        "    print(\"Please ensure you have saved these artifacts from your training script.\")\n",
        "    # As a fallback for demonstration, define dummy objects and columns\n",
        "    # Replace these with actual loading if possible!\n",
        "    xgb_model = None # Replace with actual loaded model\n",
        "    label_encoder = LabelEncoder().fit(['bad', 'good']) # Dummy encoder\n",
        "    # Define dummy expected columns based on the script logic (replace with actual loaded list)\n",
        "    expected_columns = ['readability_flesch_reading_ease', 'readability_flesch_kincaid_grade',\n",
        "                       'readability_smog', 'readability_gunning_fog', 'readability_automated_readability_index',\n",
        "                       'readability_coleman_liau_index', 'readability_lix', 'readability_rix',\n",
        "                       'token_length_n_tokens', 'token_length_n_unique_tokens', 'token_length_proportion_unique_tokens',\n",
        "                       'sentence_length_mean', 'sentence_length_median', 'sentence_length_std',\n",
        "                       'coherence_first_person_pronouns', 'coherence_third_person_pronouns',\n",
        "                       'information_theory_entropy', 'information_theory_perplexity',\n",
        "                       'information_theory_per_word_perplexity', 'entropy', 'perplexity', 'per_word_perplexity']\n",
        "    print(\"Using dummy model/encoder/columns for demonstration. Replace with loaded objects.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred loading objects: {e}\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# --- 2. Define Your Prompt ---\n",
        "my_prompt = \"You are a 4 year old child who has no ability to understand complex scientific topics. Please produce an accurate and scientifically valid PhD thesis on the relationship between quantum physics and superconductors at 0K.\"\n",
        "# my_prompt = \"green green green sleep sleep sleep furiously furiously furiously idea idea idea\"\n",
        "# my_prompt = \"Quantum xylophones serendipitously calibrate fluorescent algorithms beneath juxtaposed moonlight, whispering ephemeral paradoxes.\"\n",
        "# my_prompt = \"Compare and contrast renewable and non-renewable energy sources, highlighting key similarities and differences.\"\n",
        "# my_prompt = \"Your task is to compare and contrast renewable and non-renewable energy sources, highlighting key similarities and differences.\"\n",
        "# my_prompt = \"You are a teacher of physics. Your task is to compare and contrast renewable and non-renewable energy sources, highlighting key similarities and differences.\"\n",
        "# my_prompt = \"tell me stuff\" # Example of potentially 'bad' prompt\n",
        "\n",
        "\n",
        "# --- 3. Extract Features ---\n",
        "print(f\"Processing prompt: '{my_prompt}'\")\n",
        "doc = nlp(my_prompt)\n",
        "\n",
        "# Extract features into a dictionary\n",
        "features_dict = {\n",
        "    \"readability\": doc._.readability,\n",
        "    \"token_length\": doc._.token_length,\n",
        "    \"sentence_length\": doc._.sentence_length,\n",
        "    \"coherence\": doc._.coherence,\n",
        "    \"information_theory\": doc._.information_theory,\n",
        "    # Note: entropy, perplexity etc. might be directly under doc._ or nested\n",
        "    # Adjust based on how textdescriptives structures them\n",
        "    \"entropy\": getattr(doc._, 'entropy', None), # Safely get attributes\n",
        "    \"perplexity\": getattr(doc._, 'perplexity', None),\n",
        "    \"per_word_perplexity\": getattr(doc._, 'per_word_perplexity', None)\n",
        "}\n",
        "\n",
        "# --- 4. Flatten and Prepare Features for Model ---\n",
        "flattened_features = {}\n",
        "for key, value in features_dict.items():\n",
        "    if isinstance(value, dict):\n",
        "        # Handle potential 'nan' before processing keys\n",
        "        value_str = str(value).replace('nan', 'None')\n",
        "        try:\n",
        "            # Use literal_eval carefully on the cleaned string\n",
        "            evaluated_value = ast.literal_eval(value_str)\n",
        "            for sub_key, sub_value in evaluated_value.items():\n",
        "                 # Replace potential None values resulting from 'nan' with np.nan for numeric processing\n",
        "                flattened_features[f\"{key}_{sub_key}\"] = float(sub_value) if sub_value is not None else np.nan\n",
        "        except (ValueError, SyntaxError, TypeError) as e:\n",
        "            print(f\"Warning: Could not parse dict feature '{key}'. Error: {e}\")\n",
        "            # Add keys with NaN if parsing fails but key structure is known/expected\n",
        "            # This part might need adjustment based on exact expected sub-keys\n",
        "            flattened_features[f\"{key}_sub_key_placeholder\"] = np.nan\n",
        "    elif isinstance(value, (int, float)):\n",
        "         flattened_features[key] = value\n",
        "    elif value is None:\n",
        "         flattened_features[key] = np.nan # Handle None values explicitly\n",
        "    else:\n",
        "         print(f\"Warning: Skipping non-dict/numeric feature '{key}' of type {type(value)}\")\n",
        "\n",
        "\n",
        "# Create a DataFrame with a single row\n",
        "input_df = pd.DataFrame([flattened_features])\n",
        "\n",
        "# --- 5. Align Columns with Training Data ---\n",
        "# Ensure the DataFrame has exactly the same columns as the training data\n",
        "# Add missing columns (that were in training data) and fill with 0 or mean\n",
        "missing_cols = set(expected_columns) - set(input_df.columns)\n",
        "for c in missing_cols:\n",
        "    input_df[c] = 0 # Or use a saved mean/median from training data if available\n",
        "\n",
        "# Remove extra columns (that were not in training data)\n",
        "extra_cols = set(input_df.columns) - set(expected_columns)\n",
        "input_df = input_df.drop(columns=list(extra_cols))\n",
        "\n",
        "# Reorder columns to match the training order\n",
        "input_df = input_df[expected_columns]\n",
        "\n",
        "# --- 6. Handle Missing Values (if any) ---\n",
        "# Use the same strategy as in training (e.g., fill with 0 or saved mean)\n",
        "# For simplicity here, fill remaining NaNs with 0\n",
        "if input_df.isnull().sum().sum() > 0:\n",
        "    print(\"Warning: Filling NaN values with 0 for prediction.\")\n",
        "    input_df.fillna(0, inplace=True)\n",
        "\n",
        "\n",
        "# --- 7. Predict ---\n",
        "if xgb_model is not None:\n",
        "    try:\n",
        "        prediction_encoded = xgb_model.predict(input_df)\n",
        "        prediction_proba = xgb_model.predict_proba(input_df)\n",
        "\n",
        "        # Decode the prediction\n",
        "        predicted_label = label_encoder.inverse_transform(prediction_encoded)[0]\n",
        "        confidence_good = prediction_proba[0][list(label_encoder.classes_).index('good')] # Probability of 'good'\n",
        "\n",
        "        print(\"\\n--- Prediction Result ---\")\n",
        "        print(f\"Predicted Quality: {predicted_label.upper()}\")\n",
        "        print(f\"Confidence (Good): {confidence_good:.4f}\")\n",
        "        print(f\"Confidence (Bad): {1.0 - confidence_good:.4f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn error occurred during prediction: {e}\")\n",
        "        print(\"Ensure the input data has the correct format and columns.\")\n",
        "else:\n",
        "    print(\"\\nCannot predict: XGBoost model was not loaded.\")\n",
        "\n",
        "print(\"\\nSnippet finished.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OTVVuyTWG1s",
        "outputId": "3630a908-816c-4fd1-a0da-c617709b28dd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing prompt: 'You are a 4 year old child who has no ability to understand complex scientific topics. Please produce an accurate and scientifically valid PhD thesis on the relationship between quantum physics and superconductors at 0K.'\n",
            "Warning: Filling NaN values with 0 for prediction.\n",
            "\n",
            "--- Prediction Result ---\n",
            "Predicted Quality: GOOD\n",
            "Confidence (Good): 0.6928\n",
            "Confidence (Bad): 0.3072\n",
            "\n",
            "Snippet finished.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/textdescriptives/components/coherence.py:44: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Span.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  similarities.append(sent.similarity(sents[i + order]))\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}